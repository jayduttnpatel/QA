43
00:05:12,720 --> 00:05:15,920
low, then you reject the loan.

44
00:05:19,980 --> 00:05:25,160
If the applicant is employed, then you have
another test.

45
00:05:25,820 --> 00:05:39,750
You check the income of the applicant and
if the income is high, you approve the loan

46
00:05:39,750 --> 00:05:43,150
and if the income is low, you reject the loan.

47
00:05:45,800 --> 00:05:49,640
So, this is an example of a decision tree.

48
00:05:50,060 --> 00:05:55,480
We have three decision nodes and four leaf
nodes.

49
00:05:56,150 --> 00:05:59,650
Now, how do you use the decision tree?

50
00:06:00,060 --> 00:06:06,320
Suppose, that applicant has, is employed,
has low income.

51
00:06:06,880 --> 00:06:11,660
So, he will come here, first, the applicant
is employed.

52
00:06:11,760 --> 00:06:14,160
So, he will follow this branch.

53
00:06:14,790 --> 00:06:16,570
Check the income; income is low.

54
00:06:16,660 --> 00:06:23,020
So, he will follow this branch and the class
here is, sorry, not approved, this is reject,

55
00:06:23,610 --> 00:06:25,530
so class here is reject.

56
00:06:25,750 --> 00:06:27,950
So, this applicant will be rejected.

57
00:06:29,820 --> 00:06:36,610
So, what you want to do is that given the
training example, in the training example

58
00:06:36,610 --> 00:06:44,170
you have, for past applicants you have the
different attributes of the applicant including

59
00:06:44,170 --> 00:06:50,530
the income of the applicant, whether the applicant
is employed, what is the credit score and

60
00:06:50,530 --> 00:06:59,250
several other attributes of the applicants
are there and also the suggested action, whether

61
00:06:59,250 --> 00:07:03,450
the loan should be approved or rejected, that
is given in the training set.

62
00:07:03,450 --> 00:07:10,970
From the training set can come up the decision
tree like this so that given a new applicant,

63
00:07:10,970 --> 00:07:17,120
you can find out whether you should accept
or reject the loan.

64
00:07:17,120 --> 00:07:25,380
Similarly, you can have a decision tree to
decide whether a person is likely to buy a

65
00:07:25,380 --> 00:07:26,180
computer.

66
00:07:27,990 --> 00:07:34,550
So, let us say, you can check the age of a
person.

67
00:07:38,110 --> 00:07:40,770
Now, age is a continuous valued attribute.

68
00:07:41,460 --> 00:07:47,730
So, what you can do is, one of the things
you can do is, that you can break the entire

69
00:07:47,730 --> 00:07:50,970
age range into two or more classes.

70
00:07:51,090 --> 00:07:59,550
For example, you can say, you can divide them
into three classes: one class is age less

71
00:07:59,550 --> 00:08:05,080
than 30, another class is age less than equal
to 30.

72
00:08:05,080 --> 00:08:09,350
So, age is between 30 to 40.

73
00:08:09,350 --> 00:08:13,640
Another class is age greater than 40, right.

74
00:08:13,640 --> 00:08:23,860
So, this is how you can also accommodate continuous
variables as attributes in a decision tree.

75
00:08:23,860 --> 00:08:30,639
Now, suppose if the age is less than 30, you
have another decision variable checking if

76
00:08:30,639 --> 00:08:33,070
the applicant is a student.

77
00:08:33,070 --> 00:08:44,500
If us, if he is a student, then let us say,
we say, that he is likely to buy a computer.

78
00:08:44,500 --> 00:08:52,300
So, this is the decision tree whether an applicant,
whether a person is likely to buy a computer.

79
00:08:52,300 --> 00:08:58,000
So, if he is a student, yes; if not a student,
no, okay.

80
00:08:58,000 --> 00:09:06,090
If the age is between 30 to 40, let us say
yes, all persons between 30 to 40 are likely

81
00:09:06,090 --> 00:09:07,630
to buy a computer.

82
00:09:07,630 --> 00:09:19,050
If the age is greater than 40, like mine,
so you check the credit rating of the applicant.

83
00:09:19,050 --> 00:09:26,390
And let us say, if the credit rating is excellent,
then is not likely to buy and if the credit

84
00:09:26,390 --> 00:09:30,260
rating is fair, then he is likely to buy a
computer.

85
00:09:30,260 --> 00:09:35,820
This is another example for decision tree.

86
00:09:35,820 --> 00:09:40,440
And this is another example in the slide here.

87
00:09:40,440 --> 00:09:46,320
This is the decision tree to predict the car
mileage prediction.

88
00:09:46,320 --> 00:09:48,260
Is the weight of the car heavy?

89
00:09:48,260 --> 00:09:50,029
Yes, then high mileage.

90
00:09:50,029 --> 00:09:51,370
Is it no?

91
00:09:51,370 --> 00:09:53,520
Then, check the horsepower.

92
00:09:53,520 --> 00:09:59,620
If horsepower is less than equal to 86, then
high mileage; if no, low mileage.

93
00:09:59,620 --> 00:10:03,150
This is another example of a decision tree.