140
00:19:59,649 --> 00:20:05,440
on this we change weight this gives us stochastic
gradient descent which is passed right.

141
00:20:05,440 --> 00:20:08,860
So,we have already worked this out for linear
neurons.

142
00:20:10,240 --> 00:20:20,559
Now, the basic idea in gradient descent is
that the delta W i that we compute is equal

143
00:20:20,559 --> 00:20:31,889
to minus eta del E by del W i. So, we take
the partial derivative of the error E with

144
00:20:31,889 --> 00:20:38,090
respect to this weight and we find the direction
of the gradient and we take a small negative

145
00:20:38,090 --> 00:20:45,389
step minus given by minus eta in this direction.
This is how we change the weight at each iteration

146
00:20:45,389 --> 00:20:53,260
and if we do gradient descent on a linear
function we get function like this what we

147
00:20:53,260 --> 00:21:03,139
get is, we will get for linear which we already
worked out earlier it is a what we get is

148
00:21:03,139 --> 00:21:18,309
X i j Y j minus Y j hat. So, we get this and
we get the training rule for using gradient

149
00:21:18,309 --> 00:21:24,189
descent which is actually similar to the training
rule that we used for the perceptron.

150
00:21:26,299 --> 00:21:40,669
So, we also saw in the previous week that
for classification problem or even for other

151
00:21:40,669 --> 00:21:48,359
reasons which we will explain instead of.
So, let me explain it now only, in the next

152
00:21:48,359 --> 00:21:57,220
class we will talk about multilayer networks.
We will show that single layer networks can

153
00:21:57,220 --> 00:22:04,479
only handle linear decision surfaces, but
if we want to capture non-linear functions

154
00:22:04,479 --> 00:22:11,559
we have to go for multilayer networks. Now,
in multilayer networks we have these different

155
00:22:11,559 --> 00:22:17,720
networks connected with each other, but if
we connect linear units with each other the

156
00:22:17,720 --> 00:22:23,320
combination will again be a linear function.
So, in order to have a complex to able to

157
00:22:23,320 --> 00:22:30,299
represent complex functions, we want non-linear
unit and we want non-linear units which a

158
00:22:30,299 --> 00:22:37,229
differentiable that is why we go for a transfer
function which is differentiable and non-linear.

159
00:22:37,229 --> 00:22:46,720
And one of the transfer functions that we
often use is the sigmoid function, which is

160
00:22:46,720 --> 00:22:54,970
given by 1 by 1 plus e to the power minus
z. So, this is the logistic function or the

161
00:22:54,970 --> 00:23:01,139
sigmoid function which is one of the most
popular transfer functions which you has been

162
00:23:01,139 --> 00:23:09,759
traditionally used for a neural networks.
So, if we use this logistic unit as the transfer

163
00:23:09,759 --> 00:23:19,919
function, we can figure out how to do gradient
descent with this particular transfer function.

164
00:23:19,919 --> 00:23:28,470
So, let us say phi Z equal to 1 by 1 plus
e to the power minus z. We can differentiate

165
00:23:28,470 --> 00:23:36,029
this which we already did earlier. So, and
we found that the differentiation of this

166
00:23:36,029 --> 00:23:50,870
function can be written as phi 3 Z times 1
minus phi 3 z. So, this function is differentiable

167
00:23:50,870 --> 00:23:59,009
is differentiable and the result of the differentiation
can be very simply expressed in terms of the

168
00:23:59,009 --> 00:24:07,789
value of the function itself.
Now, with respect to logistic function, these

169
00:24:07,789 --> 00:24:17,419
sigmoid functions, let us see how to compute
the gradient. So, this is my error e equal

170
00:24:17,419 --> 00:24:33,419
to half, this is my error. So, and what is
Y d hat Y d hat is in this case sigma W dot

171
00:24:33,419 --> 00:24:45,869
X d whole square. So, we have W dot X t which
is the summation input and this pass through

172
00:24:45,869 --> 00:24:54,820
the sigmoid function. So, this is my error
function. Now, if we take the partial derivative

173
00:24:54,820 --> 00:25:11,710
of the error function with respect to the
weight W i what we get is half sigma d del