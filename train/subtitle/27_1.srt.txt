1
00:00:18,270 --> 00:00:27,270
Good morning. Today, we will look at non-linear
SVM and kernel functions in order to work

2
00:00:27,270 --> 00:00:53,530
with non-linear SVM. In the last class, we
have looked at linear SVM and we have also

3
00:00:53,530 --> 00:01:02,170
seen how to deal with noise in linear SVM.
So, we can when the data sets are actually

4
00:01:02,170 --> 00:01:07,420
linearly separable, but there is some amount
of noise in the boundary, we can approximate

5
00:01:07,420 --> 00:01:15,210
by a linear decision surface and allow soft
merging by incorporating the slack variables,

6
00:01:15,210 --> 00:01:19,950
but what is the data set is truly non-linearly
separable.

7
00:01:20,220 --> 00:01:33,020
What we can do is that we can use, suppose
x is our input features. We can use mapping

8
00:01:33,020 --> 00:01:43,729
to map x to phi x, the original features space
or original attributes space. We can transform

9
00:01:43,729 --> 00:01:51,720
into a new feature space and in some cases
it is possible that the training points are

10
00:01:51,720 --> 00:01:58,300
linearly separable in the transformed feature
space. So, usually we will be mapping data

11
00:01:58,300 --> 00:02:13,690
to a higher dimensional feature space and
in some cases it is found that when you map

12
00:02:13,690 --> 00:02:20,099
the data into an appropriate higher dimensional
feature space, in some case dimension maybe

13
00:02:20,099 --> 00:02:31,390
where infinite points become linearly separable.
Now, if we do use a big set of features then

14
00:02:31,390 --> 00:02:45,130
what about computational costs.
We have seen earlier that while solving the

15
00:02:45,130 --> 00:03:01,670
SVM, we need to find sigma alpha i alpha j
y i y j x i dot x j. So, we need to find the

16
00:03:01,670 --> 00:03:11,290
pair wise. If have m training examples, we
need to do m square computations like this

17
00:03:11,290 --> 00:03:20,180
and each time we need to find a dot product
of x i dot x j. Now, if these vectors have

18
00:03:20,180 --> 00:03:28,930
dimensionality D then finding the dot product
takes computation time O D square. So, total

19
00:03:28,930 --> 00:03:35,240
m square times D square time is taken. Now,
if we transform it to a higher dimensional

20
00:03:35,240 --> 00:03:47,510
space this x i x j will become phi x i dot
phi x j right and if this dimensionality D

21
00:03:47,510 --> 00:03:55,130
much greater than small d then this time taken
will be o capital d square which will be quite

22
00:03:55,130 --> 00:03:59,480
large.
So, normally if we transform this feature

23
00:03:59,480 --> 00:04:06,260
to a higher dimensional feature space. It
will lead to higher computational cost, but

24
00:04:06,260 --> 00:04:19,289
we will see that by using kernel function.
We can achieve this transformation without

25
00:04:19,289 --> 00:04:22,469
any major implication on computational cost.

26
00:04:23,280 --> 00:04:33,229
So, first let us see why we would we able
to convert to a different feature space where

27
00:04:33,229 --> 00:04:39,560
the points are linearly separable. Let us
look at the first picture; here we have the

28
00:04:39,560 --> 00:04:47,680
black points in a circle and the red points
outside the circle. Now, these points in the

29
00:04:47,680 --> 00:04:54,889
x 1 x 2 with x 1 x 2 dimensions they are not
separable by a linear classifier. However,

30
00:04:54,889 --> 00:05:01,590
if we go to a different feature space where
the axes are x 1 square and x 2 square, we