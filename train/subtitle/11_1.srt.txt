1
00:00:18,090 --> 00:00:22,660
Hello everyone. Welcome to the second tutorial
class of this course. I am Anirban and I am

2
00:00:22,660 --> 00:00:27,160
doing my PhD in machine learning. I am TA
for this course. In this class, we are going

3
00:00:27,160 --> 00:00:31,920
to summarize all the topics that have been
covered in the second week of this course

4
00:00:31,920 --> 00:00:36,770
and then we will study how to solve problems,
and those kinds of problems, which you can

5
00:00:36,770 --> 00:00:39,150
expect in the assignments and the exam.

6
00:00:39,790 --> 00:00:45,910
So, the topics that we are going to cover
today are linear regression and decision trees.

7
00:00:46,180 --> 00:00:52,140
So, let us take up the first topic first – linear
regression.

8
00:00:58,380 --> 00:01:06,640
As we all know, linear regression is all about
fitting a straight line to the data. And,

9
00:01:06,650 --> 00:01:12,210
in n dimensions, straight line is generalized
to a hyper plane. So, we have a set of training

10
00:01:12,210 --> 00:01:19,720
examples. Our motivation is to fit one straight
line that best fits the data, which minimizes

11
00:01:19,720 --> 00:01:30,119
the least square error in approximation. So,
you will face problems in the exam, which

12
00:01:30,119 --> 00:01:54,140
go like this. Suppose 
you have been given a set of training examples

13
00:01:54,140 --> 00:02:08,000
and they go like x 1, y 1, x 2, y 2 till x
n, y n. So, this is your given training set.

14
00:02:08,000 --> 00:02:14,990
It has n examples and these are points from
the two dimensional real space. So, we are

15
00:02:14,990 --> 00:02:32,459
going to fit one single straight line to this
data. Find the equation of the line that best

16
00:02:32,459 --> 00:02:57,520
fits the data and minimizes rather in the
sense that, it minimizes the squared error;

17
00:02:57,520 --> 00:03:04,050
all right. So, we are going find that straight
line, which minimizes the squared error and

18
00:03:04,050 --> 00:03:10,500
thus it is a best fit to the data.
So, the first step in this problem is to estimate

19
00:03:10,500 --> 00:03:27,629
the squared error. So, number 1 step is estimation
of squared error. So, this is called squared

20
00:03:27,629 --> 00:03:31,459
error, but this is actually sum of squared
error or you can also consider mean squared

21
00:03:31,459 --> 00:03:37,019
error. This does not change the solution,
but it changes the shape of the error function

22
00:03:37,019 --> 00:03:44,239
a little bit. So, the squared error function
J will look like it is equal to summation

23
00:03:44,239 --> 00:04:02,260
i equal to 1 through N y i minus f of x i
squared; where, f is your linear regression

24
00:04:06,580 --> 00:04:07,700
hypothesis.

25
00:04:09,040 --> 00:04:22,660
Now, as is very clear from the name that,
f will be of the form; think is of linear

26
00:04:22,660 --> 00:04:33,639
regression; f x is going to be of the form
m x plus c. So, rather I can write it as f

27
00:04:33,639 --> 00:04:46,210
maps from x to y. So, let this x be little
different. So, this is the set of x’s to

28
00:04:46,210 --> 00:04:56,520
the set of y’s; all right. So, then the
definition is y equal to m x plus c. So, this

29
00:04:56,520 --> 00:05:01,840
is f x; all right. So, this is the equation
of the line that you are trying to figure