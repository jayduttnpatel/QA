1
00:00:17,740 --> 00:00:26,721
Good morning. Today, we are in the third module
which is on Instance Based Learning and Feature

2
00:00:26,721 --> 00:00:33,120
Reduction. We have the Part B of this module
where we will talk about feature selection.

3
00:00:33,120 --> 00:00:39,960
In the last class we have looked at instance
based learning, and we have seen that in instance

4
00:00:39,960 --> 00:00:47,730
based learning given the test data instance
we have to find out the nearby instances for

5
00:00:47,730 --> 00:00:53,650
this we need a distance function. This distance
function is computed in term of the features.

6
00:00:53,650 --> 00:01:01,220
If the number of features is large there is
a problem, because the distance that you get

7
00:01:01,220 --> 00:01:07,070
may not be representative of the actual distance.
So, this is a reason why feature reduction

8
00:01:07,070 --> 00:01:08,650
becomes important.

9
00:01:12,810 --> 00:01:28,759
Now, we have seen that or you know that information
about the target, so features contain information

10
00:01:28,759 --> 00:01:48,520
about the target. And the function the classification
function is defined in terms of the features.

11
00:01:48,520 --> 00:02:10,420
So you may be tempted to think that more features
means better information or more information,

12
00:02:10,420 --> 00:02:24,211
and better discriminative power or better
classification power. But is this really the

13
00:02:24,211 --> 00:02:34,190
case always. We will see that this is not
always the case; this may not hold always

14
00:02:34,190 --> 00:02:41,400
that just because you have more features does
not mean you have more information or better

15
00:02:41,400 --> 00:02:43,580
classification performance.

16
00:02:45,130 --> 00:02:53,030
If you look at the slide, this is one typical
scenario. As you keep the number of training

17
00:02:53,030 --> 00:02:59,500
examples fixed and the training set is not
extremely large then typically what may happen

18
00:02:59,500 --> 00:03:05,600
is that, when you increase the number of features
initially the classifier performance may go

19
00:03:05,600 --> 00:03:13,620
up and then the classifier performance may
be degradation. So, this is the reason why can it be

20
00:03:14,190 --> 00:03:33,090
The reason is some features can be irrelevant.
In algorithm such k nearest neighbor these

21
00:03:33,090 --> 00:03:40,741
irrelevant features introduce noise and they
and they fool the learning algorithm. Because

22
00:03:40,741 --> 00:03:46,320
you are trying to find which instance is a
close together, these irrelevant features

23
00:03:46,320 --> 00:03:53,640
or noisy features will make this make the
result wrong.

24
00:03:54,320 --> 00:04:06,430
Secondly you make have redundant features.
If you have a fixed number of training examples

25
00:04:06,430 --> 00:04:13,410
and redundant features which do not contribute
additional information they may lead to degradation

26
00:04:13,410 --> 00:04:23,360
in performance of the learning algorithm.
These irrelevant features and redundant features

27
00:04:23,360 --> 00:04:36,990
can confuse learner, especially when you have
limited training examples, limited

28
00:04:36,990 --> 00:04:54,389
computational resources. When we have a large
number of features this source space the hypothesis

29
00:04:54,389 --> 00:04:58,930
space will be larger and searching may take
more time depending on the algorithm that

30
00:04:58,930 --> 00:05:06,669
you can use. And with limited training examples
you cannot work with large number features