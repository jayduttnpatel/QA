109
00:15:04,750 --> 00:15:16,540
Now, let us try to see or think about is principal
component analysis are good criteria for classification.

110
00:15:16,540 --> 00:15:22,980
So, principal component analysis if we just
have the data points not considering the label,

111
00:15:22,980 --> 00:15:28,780
the principal component gives high variation.
But, if we now look at a classification problem

112
00:15:28,780 --> 00:15:35,730
where we do not have just the input values
of the instances, we also have label is principal

113
00:15:35,730 --> 00:15:42,770
component still a good way of doing that.
To illustrate this let us look at this slide,

114
00:15:42,770 --> 00:15:50,120
we have these points and they belong to two
classes orange class and blue class. Now the

115
00:15:50,120 --> 00:15:55,520
principal component direction is given by
this pink line, and this pink line is the

116
00:15:55,520 --> 00:16:01,760
one which achieves largest variance among
the instances. But you see, this pink line

117
00:16:01,760 --> 00:16:07,480
is not very good in separating the orange
points and blue points because if you project

118
00:16:07,480 --> 00:16:12,240
the orange points and the blue points on this
axis you will see they are coming together.

119
00:16:12,240 --> 00:16:19,169
So, even through the total spread is high
this particular axis is not able to discriminate

120
00:16:19,169 --> 00:16:25,449
between the orange and blue class this means
this is not a very good idea when you look

121
00:16:25,449 --> 00:16:31,750
at a classification problem, because when
we do principal components we are not taking

122
00:16:31,750 --> 00:16:36,470
care of class information which is very important
for supervise learning.

123
00:16:38,140 --> 00:16:44,680
So, if you have these points what would be
good criteria? So, what we really want is

124
00:16:44,680 --> 00:16:51,180
a feature that separates the classes, rather
than the feature which simply has high variance.

125
00:16:51,800 --> 00:17:00,930
So, we want to make sure that the particular
feature that we choose thus separates the

126
00:17:00,930 --> 00:17:05,650
classes. If we choose this axis, it does not
separate the classes because when you project

127
00:17:05,650 --> 00:17:10,579
orange points and blue points on this axis
they will overlap. So, this is not very good.

128
00:17:10,579 --> 00:17:19,471
If we choose the pink axis, now if you project
the points on these axis what you notice is

129
00:17:19,471 --> 00:17:27,600
that the orange and blue points are separated.
So, this pink axis acts as a separator for

130
00:17:27,600 --> 00:17:36,380
the different classes rather than the blue
line here. So, if we use this pink line the

131
00:17:36,380 --> 00:17:39,760
two classes are separated. So, that is to
be preferred.

132
00:17:40,270 --> 00:17:47,050
Now, in order to achieve this we have to see
what is the information that we will use.

133
00:17:47,300 --> 00:17:54,880
First of all, we want to separate the classes,
so we want to maximize between class distances.

134
00:17:54,880 --> 00:18:03,060
So, for this we can think of that given the
points we take the centroid of the orange

135
00:18:03,060 --> 00:18:08,240
points and the centroid of the blue points
and these two should be far away. So, we wants

136
00:18:08,240 --> 00:18:15,790
the between class distance to be high.
Secondly, so this is the centroid of these

137
00:18:15,790 --> 00:18:21,630
points, this is the centroid of these points;
we want the distance between them to be high.

138
00:18:22,250 --> 00:18:30,630
Secondly, we want to, if we look at within
class distance we want within class distance

139
00:18:30,630 --> 00:18:36,300
to be small. So, within class distance is
the accumulated distance of an instance to

140
00:18:36,300 --> 00:18:42,140
the centroid of its class. So, with in the
class we want to select a feature. So, that

141
00:18:42,140 --> 00:18:47,660
upon projecting on that feature though within
class distance is small, the between class

142
00:18:47,660 --> 00:18:57,080
distance is high and linear discriminant analysis
can be used to find the most discriminant

143
00:18:57,080 --> 00:19:11,590
projection which maximizes between class distance
and minimizes within class distance. So, this

144
00:19:11,590 --> 00:19:13,650
is the representation here.

145
00:19:14,960 --> 00:19:23,840
In Linear Discriminant Analysis we define
the problem as this. Given the points belonging

146
00:19:23,840 --> 00:19:30,970
to, let us say two classes we want to find
a low dimensional space such that when we

147
00:19:30,970 --> 00:19:40,300
project the instances on this axis the classes
are well separated, they are well separated

148
00:19:40,300 --> 00:19:44,120
and we can use this following formula.

149
00:19:44,660 --> 00:19:52,220
So, m 1 is the mean of the first class, so
the centroid of class 1; m 2 is the centroid

150
00:19:52,220 --> 00:20:00,770
of the instances of class 2. S 1 square is
the standard deviation of class 1 and S 2

151
00:20:00,770 --> 00:20:03,630
square is the standard deviation of class
2.

152
00:20:05,000 --> 00:20:10,150
And we want the means to be as far away as
possible. So, the objective function that

153
00:20:10,150 --> 00:20:16,670
we use looks at the difference between the
means and we want the scatter between the

154
00:20:16,670 --> 00:20:21,970
points to be as small as possible. So, we
put scatter in the denominator.

155
00:20:23,060 --> 00:20:31,660
One particular method for doing this is Fisher
Linear Discriminant. Fisher Linear Discriminant

156
00:20:31,660 --> 00:20:39,240
uses as objective function, when you have
two classes its objective function is m 1

157
00:20:39,240 --> 00:20:48,050
minus m 2 whole square divided by s 1 square
plus s 2 square and it tries to maximize this

158
00:20:48,050 --> 00:20:55,140
objective function. It tries to find W, that
is the projection of the feature space to

159
00:20:55,140 --> 00:21:01,880
the new feature space using the parameters
W such that, this criteria is maximized.

160
00:21:02,820 --> 00:21:10,990
This is for a two class problem. These criteria
can be suitably modified when you want to

161
00:21:10,990 --> 00:21:16,100
work with 3 class, 4 class, or in general
multi class problems, but we will not talk

162
00:21:16,100 --> 00:21:21,190
about it today. So, that brings us to the
end of feature reduction.

163
00:21:21,190 --> 00:21:22,750
Thank you very much.