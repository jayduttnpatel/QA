115
00:15:04,960 --> 00:15:11,800
of uncertainty or a measure of information
content. Entropy is a very standard term which

116
00:15:11,800 --> 00:15:17,460
is used in various domains in thermodynamics,
information theory etcetera.

117
00:15:18,170 --> 00:15:24,899
In information theory just as a side, in an
information theory the optimum length code

118
00:15:24,899 --> 00:15:37,119
assigns minus log p bites. So if you have
p positive examples and p negative examples.

119
00:15:37,230 --> 00:15:49,930
So you have p positive examples and 1 minus
p negative examples. And you want to come

120
00:15:49,930 --> 00:15:59,350
up with a code then in information theory
the optimum code will use minus log p number

121
00:15:59,350 --> 00:16:12,970
of bites to send a message having probability
p. We use the idea of entropy for decision

122
00:16:12,970 --> 00:16:22,399
trees and entropy is defined as the average
optimum number of bites to encode information

123
00:16:22,399 --> 00:16:27,819
about certainty, uncertainty about s. This
is the background of entropy.

124
00:16:28,240 --> 00:16:37,480
And entropy is defined as; entropy of S. S
is a sample that is a set up examples. It

125
00:16:37,480 --> 00:16:48,770
is defined as p plus, p plus is the fraction
of positive examples in this sample. p plus

126
00:16:48,770 --> 00:17:02,860
minus log to the base 2 p plus plus p minus,
p minus is the fraction of negative examples

127
00:17:02,860 --> 00:17:16,059
in the sample, p minus negative of log 2 p
minus. Or we can write this us minus p plus

128
00:17:16,059 --> 00:17:31,820
log 2 of p plus minus p minus log 2 of p minus.
This is the definition of entropy.

129
00:17:31,820 --> 00:17:42,520
So, if it is homogeneous set, then suppose
p plus is 1 and p minus is 0. In that case

130
00:17:42,520 --> 00:17:52,330
entropy will be 0. If p plus is 0, p minus
is 1 then also entropy will be 0. And entropy

131
00:17:52,330 --> 00:17:59,590
that is the values of this function is highest
when p plus is equal to p minus equal to half.

132
00:18:00,249 --> 00:18:11,289
Now, if you look at this slide. This slide
shows with the value of p, what is the value

133
00:18:11,289 --> 00:18:18,950
of entropy. We can take e equal to p plus
in that case p minus equal to 1 minus p. So

134
00:18:18,950 --> 00:18:27,070
when p equal to - 0 entropy is 0, when p equal
to 1 entropy is 0, and when p equal to half

135
00:18:27,070 --> 00:18:39,919
entropy is 1. So, you put half here what you
get minus half log 2 half in to 2 and this

136
00:18:39,919 --> 00:18:46,830
is equal to 1. The highest value of entropy
is 1 when p plus equal to p minus equal to

137
00:18:46,830 --> 00:18:56,509
half and this is the shape of the entropy
curve. The entropy is 0 is the out coming

138
00:18:56,509 --> 00:19:00,889
certain, the entropy is maximum if you have
no knowledge of the system.

139
00:19:02,090 --> 00:19:06,470
Now based on entropy we can now define Information
Gain.

140
00:19:17,580 --> 00:19:31,909
Information gain of a sample S with respect
to an attribute A is defined to be, this is

141
00:19:31,909 --> 00:19:40,730
the original entropy and the new entropy of
the system if you split on attribute A. Suppose,

142
00:19:40,730 --> 00:19:47,260
if you split on attribute A and you get two
different values of A, then for every value

143
00:19:47,260 --> 00:19:57,809
of A this is general for all the multi values.
So for v included in all the values that A

144
00:19:57,809 --> 00:20:09,730
takes, the entropy for a particular class
this is the fraction so S v by S is the fraction