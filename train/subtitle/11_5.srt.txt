132
00:20:02,159 --> 00:20:09,049
And, in course of training a decision tree,
our motivation is to keep splitting the dataset

133
00:20:09,049 --> 00:20:19,019
into fragments, into subsets until we have
close to zero entropy. So, we should be able

134
00:20:19,019 --> 00:20:28,200
to say for certain that, given we are at a
particular leaf node, all right; the class

135
00:20:28,200 --> 00:20:34,679
distribution is almost certainly just 1.
We are completely sure that, the point belongs

136
00:20:34,679 --> 00:20:38,960
to one particular class. So, you keep start
pushing the unknown sample from the top of

137
00:20:38,960 --> 00:20:46,389
decision tree and you end up at a leaf node.
And, that particular leaf node should stand

138
00:20:46,389 --> 00:20:51,429
for one particular class. So, when the example
lands up in that particular leaf node, we

139
00:20:51,429 --> 00:20:54,820
can say that, yes, this example belongs to
that particular class, which that leaf node

140
00:20:54,820 --> 00:21:00,080
was corresponding to; right? So, this is the
notion of entropy.

141
00:21:00,730 --> 00:21:06,490
And, in connection with entropy, there is
another quantity, which is called information

142
00:21:06,490 --> 00:21:23,070
gain. So, assume that, we had an initial set
of capital N examples. Now, these kinds of

143
00:21:23,070 --> 00:21:30,350
questions are going to come in exam. And,
in this particular demonstration, I am not

144
00:21:30,350 --> 00:21:33,510
going to use numbers; I am going to just use
symbols, so that you can remember the expression

145
00:21:33,510 --> 00:21:36,170
and kind of like use it to quickly solve problems
in exam.

146
00:21:36,740 --> 00:21:42,480
And, also to get a feel of what is really
happening. So, say we are starting with capital

147
00:21:42,480 --> 00:21:58,980
N examples; and, in the initial entropy â€“ entropy
of this particular set is equal to say E 1.

148
00:21:58,980 --> 00:22:08,789
Or, rather let us say E zero or this is N
0. So, initially, we have in our set, N 0

149
00:22:08,789 --> 00:22:13,480
samples and the entropy of the set is E 0,
which was calculated in the way that I just

150
00:22:13,480 --> 00:22:19,300
demonstrated to you, using this definition.
So, the entropy was calculated. And, this

151
00:22:19,300 --> 00:22:25,059
is what we have.
Now, we chose a particular feature axis and

152
00:22:25,059 --> 00:22:34,000
a particular split on that feature axis; and,
we ended up producing these two subsets. So,

153
00:22:34,000 --> 00:22:46,640
the subsets are N 1 and N 2 large. So, N 1
plus N 2 is equal to N 0. And, the entropy

154
00:22:46,640 --> 00:23:09,870
is E 1 and E 2. So, the information gain 
is going to be calculated as E 0 minus N 1

155
00:23:09,870 --> 00:23:27,049
by N 0 into E 1 plus N 2 by N 0 into E 2.
So, this is the formula for calculation of

156
00:23:27,049 --> 00:23:34,009
information gain. And, this quantifies how
much randomness has been reduced or how pure

157
00:23:34,009 --> 00:23:39,029
the subsets become as a result of this split.
And, at every step of decision tree learning,

158
00:23:39,029 --> 00:23:43,999
we choose the feature axis and a split on
that feature axis, which maximizes the information

159
00:23:43,999 --> 00:23:48,759
gain. So, this is one of the criteria of decision
tree learning. So, in the exam, you are going

160
00:23:48,759 --> 00:23:56,080
to find questions in which you will be asked
that, which particular feature is the best

161
00:23:56,080 --> 00:24:03,040
to choose and in the context of like it for
which feature maximizes the information gain

162
00:24:03,040 --> 00:24:07,010
and thus is the best choose.
So, you have to try out for every single feature

163
00:24:07,010 --> 00:24:14,309
given in the question and calculate the information
gains associated with them. And, thus you

164
00:24:14,309 --> 00:24:22,220
can like figure out which feature is the best
and the one which maximizes the information

165
00:24:22,220 --> 00:24:27,059
gain is the best to choose. Or, you will be
given this kind of a scenario and ask to calculate

166
00:24:27,059 --> 00:24:33,149
the information gain. You will be able to
do it; right? So, this concludes the tutorial

167
00:24:33,149 --> 00:24:40,720
of this week. And, the assignment will be
released this Sunday and an announcement will

168
00:24:40,720 --> 00:24:44,999
be made in the forum. This tutorial video
will also be made available along with the

169
00:24:44,999 --> 00:24:50,510
notes.
So, the deadline will be the Thursday after

170
00:24:50,510 --> 00:24:59,100
this week. So, one and a half weeks after
the start of week 2, the deadline will be

171
00:24:59,100 --> 00:25:05,330
set. So, all of those will be announced in
the portal. And, best of luck; wish you can

172
00:25:05,330 --> 00:25:08,970
solve these kinds of questions in the exam
quite comfortably.

173
00:25:09,669 --> 00:25:10,749
Bye-bye, see you next time.