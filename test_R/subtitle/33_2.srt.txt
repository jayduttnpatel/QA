41
00:04:58,439 --> 00:05:06,279
is a problem and the slowness of convergence
is a problem, when you use a deep layer network.

42
00:05:06,279 --> 00:05:12,610
And the breakthrough that came in using deep
layer network is because people came up with

43
00:05:12,610 --> 00:05:19,050
methods which could handle this problem in
various ways some of which we will discuss

44
00:05:19,050 --> 00:05:28,430
today. First of all let us see why deep representation
is good. In many tasks, hierarchical representation

45
00:05:28,430 --> 00:05:34,990
help represent complex functions.
For example, in natural language processing,

46
00:05:34,990 --> 00:05:42,699
we have an input sentence, and we can look
at first identifying the words there is and

47
00:05:42,699 --> 00:05:47,430
we can look at the part of speech or morphology
of the word, we can then identify chunks or

48
00:05:47,430 --> 00:05:53,330
small phrases then larger phrases or clauses
and then the sentence. And we can process

49
00:05:53,330 --> 00:06:00,909
it in this order in the NLP pipeline. When
you doing image processing, you can initially

50
00:06:00,909 --> 00:06:08,360
find out the edges in the image that from
the pixels you can find the edges then the

51
00:06:08,360 --> 00:06:14,210
texton, then motifs, then parts then objects,
and if you do it hierarchically the process

52
00:06:14,210 --> 00:06:21,599
may become simpler. So in deep learning, we
are basically learning a hierarchy of internal

53
00:06:21,599 --> 00:06:22,219
representation.

54
00:06:22,599 --> 00:06:30,110
We can think of that the nodes at every hidden
layer are representing certain features. So,

55
00:06:30,110 --> 00:06:37,399
this is the input these are some low level
features, and these features are composed

56
00:06:37,399 --> 00:06:42,180
of this low level features, this is a further
compose, further compose, further compose,

57
00:06:42,180 --> 00:06:47,900
so we have a hierarchy of low level to higher
level features.

58
00:06:49,259 --> 00:06:54,499
And the learned internal representation can
be thought of as features, and we can say

59
00:06:54,499 --> 00:07:02,009
that these hidden layers are acting as feature
extractor. So, deep learning can be looked

60
00:07:02,009 --> 00:07:10,889
upon as feature learning. And one way of looking
at it is that we have the input, and here

61
00:07:10,889 --> 00:07:28,659
we have a trainable feature extractor. So,
these are all feature extractors. And finally,

62
00:07:28,659 --> 00:07:37,039
at the output, we have a classifier, if you
have a classification problem.

63
00:07:39,710 --> 00:07:47,580
Now one of the ways in which deep networks
can be trained is by using unsupervised pre-training.

64
00:07:47,580 --> 00:07:56,080
There are several ways one is that you could
try to setup the units, so that the gradient

65
00:07:56,080 --> 00:08:05,629
diffusion or gradient blowing a problem can
get reduced. And there are some ways of using

66
00:08:05,629 --> 00:08:12,300
activation functions other than sigmoid functions.
For example, people use a function called

67
00:08:12,300 --> 00:08:20,029
Relu, which we will see which avoids the problem
of gradient diffusion. The other approach

68
00:08:20,029 --> 00:08:28,709
is using unsupervised pre-training, so instead
of learning a deep network at one go, we learn

69
00:08:28,709 --> 00:08:37,940
the network one-step at a time using a not
to deep network. And by growing the layers

70
00:08:37,940 --> 00:08:44,600
one at a time, we can avoid the problem which
deep networks have.

71
00:08:44,940 --> 00:08:52,660
If you look at this slide, we show the schematic
of doing greedy layer wise pre-training. This

72
00:08:52,660 --> 00:09:01,100
is the input. We first learn the first hidden
layer by using some mechanism; one of the

73
00:09:01,100 --> 00:09:07,490
mechanisms is auto encoder, which I will discuss
briefly today. Another is by using restricted

74
00:09:07,490 --> 00:09:13,590
Boltzmann machine. So, by several such approaches
is possible to first learn this layer.

75
00:09:13,700 --> 00:09:20,390
When you learn this layer, you fix this layer;
after you fix this layer in terms of this

76
00:09:20,390 --> 00:09:26,850
you learn the next layers, then you fix it,
and then you learn the next layer and so on.

77
00:09:27,650 --> 00:09:35,770
So, here you are fixing the parameters of
the previous hidden layers, and you are assuming

78
00:09:35,770 --> 00:09:42,220
that these are different features, and then
you are trying to find the next layer. So

79
00:09:42,220 --> 00:09:46,280
this is the idea of greedy layer wise pre-training.

80
00:09:46,280 --> 00:09:52,930
After you have learnt the layers individually,
finally you can look at the classification

81
00:09:52,930 --> 00:10:00,650
problem. And based on the training example,
you can setup the classifier at the last layer