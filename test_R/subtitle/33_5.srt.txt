168
00:20:01,100 --> 00:20:08,669
be learned this is another way of imposing
a constraint. And we are as we saw that we

169
00:20:08,669 --> 00:20:16,440
are stacking different layers. It has been
found that this spars autoencoders can be

170
00:20:16,440 --> 00:20:21,470
stacked easily, whereas the previous constraint
where you can strain the number of hidden

171
00:20:21,470 --> 00:20:28,230
units, they do not after you know they are
not so much amenable to stacking.

172
00:20:28,230 --> 00:20:36,730
So, we can add such constraints, and then
we can put the input here, input here, learn

173
00:20:36,730 --> 00:20:42,450
the first hidden layer. We can fix the weights
from the input to the first hidden layer.

174
00:20:42,450 --> 00:20:49,270
And then we can after fixing the weights then
at the end we can learn the classifier, of

175
00:20:49,270 --> 00:20:53,050
course, we can do more than one hidden layer.

176
00:20:56,130 --> 00:21:00,600
And by stacking autoencoder we can use more
than one hidden layer. This is the first hidden

177
00:21:00,600 --> 00:21:06,460
layer then, there is a second hidden layer.
So, we do supervise, so we do layer by layer

178
00:21:06,460 --> 00:21:16,440
using unlabeled examples. Finally, we do supervise
training on the output layer and then we tune

179
00:21:16,440 --> 00:21:25,110
the entire network. So, this is the idea of
stacked autoencoder and this is one way of

180
00:21:25,110 --> 00:21:33,860
learning a deep network, where we avoid the
problems the standard problems that is where

181
00:21:33,860 --> 00:21:44,049
they are with deep networks. There is other
as I said we have stacked RBM, where we use

182
00:21:44,049 --> 00:21:51,850
energy minimization to learn layer by layer
using bidirectional neural network, but that

183
00:21:51,850 --> 00:21:53,850
we will not discuss in this class.

184
00:21:55,279 --> 00:22:03,289
Now we will briefly talk about another approach
to deep neural networks using convolutional

185
00:22:03,289 --> 00:22:12,499
neural network. Convolutional neural network
has been very popular for working with images.

186
00:22:12,499 --> 00:22:18,320
And now it has also been applied to other
radius including natural language process.

187
00:22:18,320 --> 00:22:25,059
So, convolutional neural network consists
of a number of layers, and these layers have

188
00:22:25,059 --> 00:22:33,460
some specific functions. They are two types
of layers, which are very popular convolutional

189
00:22:33,460 --> 00:22:41,049
layer and subsampling or pooling layer. The
input so convolutional layer is an image.

190
00:22:41,049 --> 00:22:46,799
Suppose, you know it could be another thing,
but let us talk about an image. Suppose, there

191
00:22:46,799 --> 00:22:54,179
is an image, which consists of m by m pixels;
and for each pixel, we may have a number of

192
00:22:54,179 --> 00:23:00,600
channels. For example, if it is an r g b image,
there are three channels. So the number of

193
00:23:00,600 --> 00:23:09,729
inputs corresponding to the image is m by
m by r. So, these are the different channels

194
00:23:09,729 --> 00:23:23,019
- r g b, so this is the image. And the convolutional
layer we use k filters, so use k functions;

195
00:23:23,019 --> 00:23:32,299
and each of this functions will do a mapping.
So, let us explain how this functions convolution

196
00:23:32,299 --> 00:23:32,939
function works.

197
00:23:33,879 --> 00:23:43,260
For simplicity, let us assume we have m by
m image and there is only one channel. Now

198
00:23:43,260 --> 00:23:50,279
in convolution what we will do is that we
will take a sub rectangle. Suppose, we take

199
00:23:50,279 --> 00:24:06,029
4 by 4 sub rectangle, then we can take different
4 by 4 sub rectangles. Now corresponding to

200
00:24:06,029 --> 00:24:18,239
this sub rectangle we have 16 inputs, and
this 16 inputs will feed to this corner this

201
00:24:18,239 --> 00:24:28,989
is k 1 is one feature. And if there are w
1 to w 16, these are the 16 weights, so this

202
00:24:28,989 --> 00:24:35,529
is an output.
Now what we will do is that for other sub

203
00:24:35,529 --> 00:24:43,330
rectangles also we will go to this function
K 1, and they will also share the same weight

204
00:24:43,330 --> 00:24:54,779
W 1, W 2, W 16. So, what we do is that in
convolution for every node, we will for every

205
00:24:54,779 --> 00:25:03,450
rectangle we will so for a particular kernel
that we are using for every rectangle we will