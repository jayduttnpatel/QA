101
00:15:12,829 --> 00:15:19,129
we want to find how close they are. I think
all of you know about the Euclidean distance.

102
00:15:19,430 --> 00:15:25,190
In Euclidean distance, how do we measure the
distance between these points. We say that

103
00:15:25,190 --> 00:15:53,540
distance Euclidean x i, x j is given by root
over x i k minus x j k whole square sigma

104
00:15:53,540 --> 00:16:06,300
k equal to 1 to n. So, this is the Euclidean
distance. So, we can find the Euclidean distance

105
00:16:06,300 --> 00:16:12,790
from a test point to all the points that we
have got from training and select that point

106
00:16:12,790 --> 00:16:21,080
which has the smallest Euclidean distance.
When we take k as to be more than 1, we can

107
00:16:21,080 --> 00:16:27,759
take if itâ€™s a regression problem.
We can take the average of the values, when

108
00:16:27,759 --> 00:16:36,690
do we go for averaging with large k or when
do we go for k equal to 1. So, we will go

109
00:16:36,690 --> 00:16:52,449
for averaging under circumstances where there
are noises in attributes. So, because of the

110
00:16:52,449 --> 00:16:59,220
noise, the instance that is closest to the
test instance mat not capture everything about

111
00:16:59,220 --> 00:17:04,770
the test instance in a better way than any
other instance which slightly further way,

112
00:17:04,770 --> 00:17:20,630
but still close than there can noise in class
labels and thirdly classes may be over lapping.

113
00:17:22,540 --> 00:17:39,000
So, under such circumstances, there is a case
for using k larger than 1 and we will see

114
00:17:39,000 --> 00:17:53,070
that when we use a larger value k, we get
a better smoother classifier. Now, the second

115
00:17:53,070 --> 00:18:01,630
thing we will worry about is when we look
at the Euclidean distance, we are taking some

116
00:18:01,630 --> 00:18:06,500
of square root over some of squares of the
distance for each attribute.

117
00:18:06,500 --> 00:18:14,210
Let us not use k here because k is used for
k nearest neighbor, but here we are talking

118
00:18:14,210 --> 00:18:31,770
about an index variable. So, let us use m
now. All attributes are not equally important

119
00:18:31,770 --> 00:18:40,940
or the attributes may have different case.
So, a normal Euclidean distance treats all

120
00:18:40,940 --> 00:18:49,630
the attributes at the same scale, but there
may be case for giving different ways to the

121
00:18:49,630 --> 00:18:55,500
attribute. When you are giving equal weights
to the attribute, you can give equal weights

122
00:18:55,500 --> 00:18:59,640
only under certain circumstances or certain
assumptions.

123
00:19:04,520 --> 00:19:32,390
Should we give equal weights to all attributes?
Now if you can do only if the scales of the

124
00:19:32,390 --> 00:19:46,640
attributes are similar. What do you mean by
scale? Suppose height is an attribute and

125
00:19:46,640 --> 00:19:52,480
if you are measuring height in centimeter
verse height in feet. The differences are

126
00:19:52,480 --> 00:19:59,820
not the same, right. So, you have the different
attributes depending on what scale you use