90
00:10:01,970 --> 00:10:08,629
syntax in Python 2, you have to do the import
from future and then, this goes like this.

91
00:10:08,629 --> 00:10:12,459
So, this is going to be, so this is a blank
space holder, it is going to be filled by

92
00:10:12,459 --> 00:10:16,449
accuracy. So, much of accuracy has been obtained
with kernel this.

93
00:10:17,100 --> 00:10:22,080
So, we are going to, like, test every single
kernel one by one in this particular section.

94
00:10:22,170 --> 00:10:28,889
So, let us go ahead and run this, cool. So,
here we have the results and see that the

95
00:10:28,889 --> 00:10:34,980
accuracies of the linear and RBF kernels are
equal. And this is the particular case over

96
00:10:34,980 --> 00:10:44,589
here not always is the same case and the accuracy
of the polynomial kernel is little bit lower.

97
00:10:44,589 --> 00:10:54,360
So, let us now go ahead and try and visualize
the decision boundaries. So, this will give

98
00:10:54,360 --> 00:10:59,360
a good insight into how the problem is being
solved by the support vector classifier. So,

99
00:10:59,360 --> 00:11:04,759
let us just go ahead and look at the code.
So, this particular section visualizes the

100
00:11:04,759 --> 00:11:12,329
decision boundaries. So, the first segment
we are going to train different kinds of classifier

101
00:11:12,329 --> 00:11:18,189
so that we train them all over once more.
So, we just name them as SVC.

102
00:11:18,259 --> 00:11:24,060
So, the simple svc, this model is going to
be a linear kernel SVM and it is it has been

103
00:11:24,060 --> 00:11:32,180
trained on the training data. The second RBF
SVC is trained is an RBF kernel SVM and it

104
00:11:32,180 --> 00:11:36,350
has been trained. So, gamma is another parameter
of the sum. So, you can actually look up the

105
00:11:36,350 --> 00:11:41,050
documentation of support vector machine dot
support vector classifier of Scikit Learn

106
00:11:41,050 --> 00:11:45,940
and you will find all of those, like the definitions
of each of these parameters and then, you

107
00:11:45,940 --> 00:11:50,240
train it on the training data. And now, you
have a polynomial kernel SVM in the same way.

108
00:11:50,240 --> 00:11:55,329
So, here you are going to, so in this section
I will highlight, so in this section you are

109
00:11:55,329 --> 00:12:00,129
going to initialize different kinds of kernels
and train them.

110
00:12:00,850 --> 00:12:06,740
Now, we would like have to have a mesh plot,
which will look beautiful. So, that is why

111
00:12:06,740 --> 00:12:13,720
you are, in this section you create a mesh
of points. So, it extends from the minimum

112
00:12:13,720 --> 00:12:18,420
possible value of the training set, of each
feature of the training set to their maximum

113
00:12:18,420 --> 00:12:26,009
possible values. So, x min and x max have
been calculated, right, and a bit, and like

114
00:12:26,009 --> 00:12:32,029
one point earlier and one point later. So,
this code is just for like nice plot and then

115
00:12:32,029 --> 00:12:40,459
you define the mesh using NumPy dot meshgrid.
And now, you go ahead and discuss and we are

116
00:12:40,459 --> 00:12:46,709
making plots. So, the titles will be like
this. So, you are making an array of these

117
00:12:46,709 --> 00:12:56,160
titles and then, you again, like, go ahead
and evaluate different modules. So, it is

118
00:12:56,160 --> 00:13:02,480
like in this particular line what we are going
to do is, we are again enumerating over this

119
00:13:02,480 --> 00:13:09,709
tuple. So, all of these three models separated
by commas within first brackets make a tuple,

120
00:13:09,709 --> 00:13:17,709
which is iterable and when you use enumerate
on this, then it returns an index of each

121
00:13:17,709 --> 00:13:23,699
element. It returns each element one by one,
returns the element one by one. It returns

122
00:13:23,699 --> 00:13:27,119
in the index of the element as well as the
element itself.

123
00:13:27,480 --> 00:13:34,920
So, we are going to have svc first with an
index of 0, i equal to 0 and then, RBF SVC

124
00:13:34,920 --> 00:13:45,230
and then, poly SVC and then, you first initialize
the figure and you do all the predict for

125
00:13:45,230 --> 00:13:52,639
all the points in the mesh. So, what we are
trying to do over here is find out which part

126
00:13:52,639 --> 00:13:58,370
of the space belongs to which particular category
and you will see in a second how this thing

127
00:13:58,370 --> 00:14:08,679
actually looks. So, then you put everything
into a contour plot and finally, place the

128
00:14:08,679 --> 00:14:18,589
points of the dataset in the plot and do the
labeling of the axis over here and then do

129
00:14:18,589 --> 00:14:26,670
some more cosmetic changes to the plot and
then, you put the titles on the plots. And

130
00:14:26,670 --> 00:14:30,670
finally, you show all the plots. So, let us
go ahead and run this part.

131
00:14:31,069 --> 00:14:36,540
See how beautiful it is. So, it generates
these three plots and the 1st one is for the

132
00:14:36,540 --> 00:14:40,910
linear kernel, the 2nd one is for the RBF
kernel and 3rd one is for the polynomial kernel.

133
00:14:40,910 --> 00:14:46,570
So, let us go ahead and inspect these one
by one and compare. So, in the linear kernel,

134
00:14:46,570 --> 00:14:53,430
linear kernel is nothing, but just a linear
decision module, right, like no future transmissions.

135
00:14:53,610 --> 00:15:00,470
The entire idea of kernels is like this. You
have this set of, like, you may have a set