1
00:00:18,859 --> 00:00:21,299
Good morning, I am Sudeshna Sarkar.

2
00:00:21,570 --> 00:00:24,530
Today, we start the first lecture on machine
learning.

3
00:00:25,359 --> 00:00:32,460
This is module one, part A. Today, we will
introduce machine learning, basic, go through

4
00:00:32,460 --> 00:00:39,760
the basics of the course, discuss the brief
history of machine learning and discuss what

5
00:00:39,760 --> 00:00:45,220
learning is about and some simple applications
of machine learning.

6
00:00:46,800 --> 00:00:49,900
First, this is the overview of the course.

7
00:00:50,380 --> 00:00:54,560
The course is over 8 weeks and will have 8
modules.

8
00:00:54,680 --> 00:01:00,600
The 1st module is Introduction; in the 2nd
module we will discuss about Linear Regression

9
00:01:00,610 --> 00:01:07,540
and Decision trees; 3rd module, Instance Based
Learning and Feature selection; 4th module,

10
00:01:07,540 --> 00:01:13,860
Probability and Bayes learning; 5th module,
Support Vector Machines; 6th module, Neural

11
00:01:13,860 --> 00:01:19,420
Networks; 7th module, we will do an Introduction
to Computational learning theory and possibly

12
00:01:19,420 --> 00:01:25,460
a little bit on sample learning and then,
the last module we will talk about Clustering.

13
00:01:25,460 --> 00:01:29,310
In the 1st module, that is introduction, we
will have four parts.

14
00:01:29,310 --> 00:01:35,500
Today, we will give a brief introduction,
in the next more lectures we will have discuss

15
00:01:35,500 --> 00:01:40,300
about different types of learning supervise
and supervised, etcetera.

16
00:01:40,390 --> 00:01:45,670
Then, we will have the 3rd module where we
will talk about hypothesis space and inductive

17
00:01:45,670 --> 00:01:46,230
bias.

18
00:01:46,670 --> 00:01:52,970
Following this, we will talk about evaluation
training and test set and cross-validation.

19
00:01:53,240 --> 00:01:58,900
First, I will like to start with a brief history
of machine learning.

20
00:01:59,869 --> 00:02:08,020
A machine that is intellectually capable as
much as humans has always fired the imagination

21
00:02:08,020 --> 00:02:16,900
of writers and also the early computer scientist
who were excited about artificial intelligence

22
00:02:16,900 --> 00:02:24,220
and machine learning, but the first machine
learning system was developed in the 1950s.

23
00:02:25,040 --> 00:02:29,160
In 1952, Arthur Samuel was at IBM.

24
00:02:29,850 --> 00:02:32,490
He developed a program for playing Checkers.

25
00:02:33,030 --> 00:02:42,120
The program was able to observe positions
at the game and learn a model that gives better

26
00:02:42,120 --> 00:02:44,560
moves for the machine player.

27
00:02:45,180 --> 00:02:51,420
The system played many games with the program
and observed that the program was able to

28
00:02:51,420 --> 00:02:58,380
play better in the course of time with getting
more experience of board games.

29
00:02:59,340 --> 00:03:05,620
Samuel coined the term machine learning and
he defined learning as a field of study that

30
00:03:05,620 --> 00:03:11,040
gives computers the ability without being
explicitly programmed.

31
00:03:12,120 --> 00:03:18,640
In 1957, Rosenblatt proposed the perceptron.

32
00:03:19,000 --> 00:03:26,060
Perceptron is the simple neural network unit;
it was a very exciting discovery at that time.

33
00:03:26,069 --> 00:03:33,070
Rosenblatt made the following statement; the
perceptron is designed to illustrate some

34
00:03:33,070 --> 00:03:40,490
of the fundamental properties of intelligent
systems in general without becoming too deeply

35
00:03:40,490 --> 00:03:47,630
immersed in the special and frequently unknown
conditions, which hold force particular biological

36
00:03:47,630 --> 00:03:48,310
organisms.

37
00:03:49,750 --> 00:03:58,980
But after 3 years, came up with the delta
learning rule that is used for learning perceptron.

38
00:03:58,980 --> 00:04:02,820
It was used as a procedure for training perceptron.

39
00:04:02,820 --> 00:04:05,680
It is also known as the least square problem.

40
00:04:06,490 --> 00:04:11,090
The combination of these ideas created a good
linear classifier.

41
00:04:12,680 --> 00:04:23,569
However, the work along these lines suffered
a setback when Minsky in 1969 came up with

42
00:04:23,569 --> 00:04:25,569
the limitations of perceptron.

43
00:04:26,430 --> 00:04:35,029
He showed, that the problem could not be represented
by perceptron and such inseparable data distributions

44
00:04:35,029 --> 00:04:44,009
cannot be handled and following this Minskyâ€™s
work neural network research went dormant

45
00:04:44,009 --> 00:04:45,809
up until the 1980s.

46
00:04:48,249 --> 00:04:57,210
In the meantime, in the 1970s, machine learning
symbolic following the symbolic type of artificial

47
00:04:57,210 --> 00:05:02,499
intelligence, good old fashioned artificial
intelligence, those types of learning algorithms