158
00:25:07,890 --> 00:25:33,890
and this may be necessary if the training
set is small. On the other hand, you can use

159
00:25:33,890 --> 00:25:44,260
large value of k under the following circumstances.
Let us first look at an example. On the same

160
00:25:44,260 --> 00:25:53,950
data set using 15 nearest neighbor, what you
notice here is that the classes are smoother.

161
00:25:53,950 --> 00:26:00,570
The neighboring classes in here, all of them
are classified as red rather than blue for

162
00:26:00,570 --> 00:26:13,950
50 nearest neighbor. So, use large k under
these circumstances. When use large k, the

163
00:26:13,950 --> 00:26:26,520
classifier that you get is less sensitive
to noise, particularly noise in the output

164
00:26:26,520 --> 00:26:50,520
class. So, particularly class noise, you get
better probability estimates for discreet

165
00:26:50,520 --> 00:27:06,660
classes 
and you can use for discrete classes. If the

166
00:27:06,660 --> 00:27:11,810
classes are discrete, you get better probability
estimates if use large values of k.

167
00:27:12,440 --> 00:27:20,080
Thirdly, if you have larger size of training
set, then you can use large values of k. So,

168
00:27:20,080 --> 00:27:38,280
larger training set allows you to use larger
k. If you have very small training set size,

169
00:27:38,280 --> 00:27:46,050
you cannot use large k. You have to use small
k. So, this is the basic idea of getting different

170
00:27:46,050 --> 00:27:50,610
values of k. Now, let us look at the diagram
on the slide.

171
00:27:51,300 --> 00:28:00,290
So, this picture shows you know this shows
the number x axis is the number of neighbors.

172
00:28:00,290 --> 00:28:09,610
The blue line is test error, the green line
is ten-fold cross validation error and the

173
00:28:09,610 --> 00:28:17,030
red line is training error and the dashed
line is base error which is close to the true

174
00:28:17,030 --> 00:28:26,120
error. We see that in the training set as
we are increasing the value of k, you know

175
00:28:26,120 --> 00:28:35,600
initially the error will increase and then,
error will after that remain all most steady,

176
00:28:35,600 --> 00:28:43,780
but for the test error and for the cross validation
error, we see that for k equal to 1, for one

177
00:28:43,780 --> 00:28:49,950
nearest neighbor, the error is high and then,
the error falls off.

178
00:28:49,950 --> 00:28:56,250
After sometime the error rises and then, keeps
almost fixed. So, this is the region where

179
00:28:56,250 --> 00:29:01,780
the test error is smallest and this is region
where the cross validation error is smallest.

180
00:29:01,780 --> 00:29:08,060
So, in this particular example, the error
is smallest that around k equal to 5 or k

181
00:29:08,060 --> 00:29:14,220
equal to 7. So, refer small k, the error is
high when large scale also, it increases and

182
00:29:14,220 --> 00:29:20,420
some middle value k, you have the best performance
if you look at the test error.

183
00:29:20,420 --> 00:29:27,890
Now, we come to the second question. The issue
was using different value of k. The second

184
00:29:27,890 --> 00:29:36,490
issue was to use weighted distance function.Now, we will see how we can use weighted distance function

185
00:29:37,240 --> 00:29:42,460
In the case of weighted distance function,
what we do is that we define the distance.

186
00:29:43,090 --> 00:29:55,010
You know let me write it in our notation distance
between x i and x j depends on different attributes

187
00:29:55,010 --> 00:30:00,770
with different weights. We have n attributes
and their weights w 1, w 2 and w n. So, we