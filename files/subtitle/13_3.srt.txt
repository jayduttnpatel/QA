59
00:10:00,970 --> 00:10:09,269
the number of sub sets is exponentially. So
we need a method which works in reasonable

60
00:10:09,269 --> 00:10:13,060
time.
The methods that we can use for feature subsets

61
00:10:13,060 --> 00:10:30,560
selection can be Optimum methods, can be Heuristic
methods, and can be Randomized methods. However,

62
00:10:30,560 --> 00:10:39,000
we can use optimum methods if the hypothesis
space or the feature subset space has a structure

63
00:10:39,000 --> 00:10:45,980
so that we can have a optimum algorithm which
works in polynomial time. Otherwise, we can

64
00:10:45,980 --> 00:10:50,610
use a heuristic or greedy algorithm or some
type of randomized algorithm.

65
00:10:50,610 --> 00:10:57,160
Now, the search algorithm which considers
the different feature subsets will also have

66
00:10:57,160 --> 00:11:08,550
some mechanism to evaluate it subset. And
for evaluation we use two types of methods

67
00:11:08,550 --> 00:11:20,720
which I will describe in detail; Unsupervised
methods, and Supervised methods. In unsupervised

68
00:11:20,720 --> 00:11:32,279
methods, we do not evaluate the subsets over
the training examples. We evaluate the information

69
00:11:32,279 --> 00:11:41,801
content in somewhere in an unsupervised way
these methods are called Filter methods. In

70
00:11:41,801 --> 00:11:53,220
supervised methods also called the Wrapper
methods, we evaluate the feature subset by

71
00:11:53,220 --> 00:12:00,310
using it on a learning algorithm. These are
called supervised methods or wrapper methods.

72
00:12:00,310 --> 00:12:21,571
Now let us look at, as I said the features
selection is an optimization problem. You

73
00:12:21,571 --> 00:12:26,769
have a search algorithm, you have an objective
function, and you are trying to optimize the

74
00:12:26,769 --> 00:12:34,050
objective function. If you look at this diagram
here the search algorithm will select a feature

75
00:12:34,050 --> 00:12:40,350
subset and score it on the objective function
and find the goodness of the feature subset

76
00:12:40,350 --> 00:12:46,750
based on this it will decide which part of
the search space to explore next. And after

77
00:12:46,750 --> 00:12:53,990
this module is completed you get a final feature
sub set and this final feature subset is use

78
00:12:53,990 --> 00:12:59,850
by your machine learning or pattern recognition
algorithm. So you want to pick the subset

79
00:12:59,850 --> 00:13:04,950
that is optimum or near optimal with respect
to the objective function.

80
00:13:06,290 --> 00:13:15,839
Now, as a features subset can be evaluated
by two methods in supervise methods or wrapper

81
00:13:15,839 --> 00:13:23,019
methods we train using the selected subset
and we estimates the error on the validation

82
00:13:23,019 --> 00:13:31,670
set. In unsupervised or filter methods we
look at only the input and we select the subset

83
00:13:31,670 --> 00:13:33,790
which has the most information.

84
00:13:34,149 --> 00:13:42,019
Now, these two types of methods are illustrated
in this picture. In the filter method the

85
00:13:42,019 --> 00:13:48,699
search algorithm comes up with a feature subset
that is evaluated for information context

86
00:13:48,699 --> 00:13:54,889
and based on that search algorithm proceeds
and finally this module gives a final features

87
00:13:54,889 --> 00:14:02,230
subset. In the wrapper based method or supervised
method the search algorithm output a sub features

88
00:14:02,230 --> 00:14:08,800
subset which is again used with the pattern
recognition or machine learning algorithm

89
00:14:08,800 --> 00:14:14,329
and the prediction accuracy is obtained which
is fed to the search algorithm. After this

90
00:14:14,329 --> 00:14:19,720
whole module is completed you have a feature
subset which is used by your machine learning

91
00:14:19,720 --> 00:14:26,630
algorithm.
So, these are two different frameworks of

92
00:14:26,630 --> 00:14:36,589
feature selection. Now how do you do the feature
selection algorithm? First of all what you

93
00:14:36,589 --> 00:14:44,709
can do is that, if features are redundant
you may use all the features.

94
00:14:46,029 --> 00:15:00,791
So, you can find uncorrelated features. You
start with some features and then when then